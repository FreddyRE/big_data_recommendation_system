version: "3.8"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"   # NN Web UI
      - "8020:8020"   # NN RPC
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=3
      # Correct NN addresses + bind hosts
      - HDFS_CONF_dfs_namenode_rpc___address=namenode:8020
      - HDFS_CONF_dfs_namenode_rpc__bind__host=0.0.0.0
      - HDFS_CONF_dfs_namenode_http___address=0.0.0.0:9870
      - HDFS_CONF_dfs_namenode_http__bind__host=0.0.0.0
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false\
      - HDFS_CONF_dfs_namenode_heartbeat___recheck___interval=10000   # 10s
      - HDFS_CONF_dfs_namenode_stale_datanode_interval=10000          # 10s
    volumes:
      - namenode:/hadoop/dfs/name
    platform: linux/amd64
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://localhost:9870 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=3
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - SERVICE_PRECONDITION=namenode:9870
      - HDFS_CONF_dfs_heartbeat_interval=1    
    # IMPORTANT: no shared volume here if you plan to scale
    # (each container will use its own internal storage)
    platform: linux/amd64
    expose:
      - "9864"
      - "9866"
      - "9867"

volumes:
  namenode: